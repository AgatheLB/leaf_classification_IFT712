\section{Conception}
Le choix des différents classifieurs pour mener à bien ce projet a été réalisé dans l'optique d'essayer plusieurs familles de classifieurs. Nous nous sommes arrêtés à l'utilisation de six classifieurs multiclasses pour une application d'un apprentissage supervisée implémentés par la librairie Sklearn.

\subsubsection*{Perceptron MultiCouches}
Le réseau de neurones de type Perceptron MultiCouches a pour but d'apprendre à établir un lien mathématique entre les données de l'ensemble d'entrée et les différentes étiquettes de classe, dans le cas d'une classification. De part la possibilité de disposer de multiples couches, le réseau a la capacité d'établir des modèles non-linéaires, qui pourrait se révéler intéressant sur notre jeu de données. Nous décidons d'utiliser la méthode \emph{neural\_network.MLPClassifier} implémentée par Sklearn, qui utilise la rétro-propagation ainsi que la fonction d'erreur de l'entropie croisée. Aussi, étant donné notre problème multi-classe, la fonction d'activation de la couche de sortie est une Softmax.

\subsubsection*{K-Plus Proches Voisins}
La classification basée sur les voisins n'a pas comme but d'être la méthode la plus généralisable possible, elle se contente en effet de sauvegarder les exemples du jeu de données d'entraînement. Celle-ci peut cependant se révélée très performante et, au vu de la taille de notre jeu de données d'entraînement, son utilisation est envisageable.
Nous avons décidé de tester la méthode des K-Plus Proches Voisins, implémentés par Sklearn avec neighbors.KNeighborsClassifier.

\subsubsection*{Analyse du Discriminant Linéaire}
L'essai sur notre jeu de données d'entraînement de la méthode de l'analyse du discriminant linéaire permet de s'informer si ce jeu de données est effectivement bien linéairement séparable. Ce type de classification, étant de type \textcolor{red}{closed-form}, démontre son efficacité lors de la phase d'entraînement. La méthode est implémentée par Sklearn sous la librairie \emph{discriminant\_analysis.LinearDiscriminantAnalysis}.

\subsubsection*{Bayes naïf gaussienne}
Les méthodes de classification se basant sur l'application du théorème de Bayes font l'hypothèse qu'il exsite une indépendance conditionnelle entre chaque attribut sachant la valeur de l'étiquette de classe. L'utilisation d'une estimation par Maximum A Posteriori est aussi utilisée afin de déterminer les distributions de chacune des étiquettes de classe et de chacun des attributs en sachant la valeur de l'étiquette de classe. Nous avons décider, en plus, de faire l'hypothèse que les données suivent des distributions gaussiennens dans l'optique d'utiliser la méthode de classification de Bayes naïf gaussien. Celle-ci se retrouve implémentée par Sklearn sous la méthode \emph{naive\_bayes.GaussianNB}.

\subsubsection*{Forêts aléatoires}
Dans le but de généraliser au mieux, les méthodes d'ensemble permettent de combiner les prédictions de multiples estimateurs en un seul estimateur. On s'intéresse tout d'abord aux méthodes basées sur la moyenne des prédictions de chaque estimateur, dont les arbres de décision aléatoire font partis. 
La méthode Sklearn \emph{ensemble.RandomForestClassifier} a la particularité d'implémenter une méthode de type Boostrap, qui permet de décroître d'autant plus la variance de l'estimateur ayant souvent tendance à sur-apprendre.

\subsubsection*{Machine à vecteur de support}
Les méthodes de machines à vecteur de support sont également implémentées dans Sklearn et permettent d'apporter une approche "un-contre-un" à la classification. Celles-ci montrent aussi de bons résultats lorsque la dimensionalité des données est importante, ce qui pourrait être le cas de notre jeu de données d'entraînement. On utilise alors la méthode Sklearn \emph{svm.SVC} qui est spécifique à la classification.

\subsection{Choix des métriques}
Afin de déterminer les hyper-paramètres menant à des performances maximales des modèles de classification (meilleure justesse d'entraînement et de validation), nous avons décidé d'utiliser une méthode de validation croisée. 

Pour cela, nous avons pu la méthode Sklearn \emph{model\_selection.GridSearchCV}, qui prend en paramètre un estimateur (notre modèle de classification), un espace de paramètres à tester, une fonction d'évalutation (dans notre cas, la justesse).

On s'intéresse maintenant aux hyper-paramètres des différents modèles pour notre validation croisée. 

\subsubsection*{Perceptron MultiCouches}
\begin{itemize}
	\item \textbf{Taille des couches cachées}: Facteur faisant varier le nombre de neurones composant une couche cachée.
	\item \textbf{Initialisation des taux d'apprentissage}: Valeur du taux d'apprentissage à l'initialisation du réseau. 
	\item \textbf{Algorithme d'optimisation}: Choix de l'algorithme d'optimisation pour les mises à jours des poids du réseau (\emph{adam}, \emph{sgd}.
	\item \textbf{Fonction d'activation}: Choix de la fonction d'activation utilisée par les couches cachées (\emph{relu}, \emph{logistic}).
\end{itemize}

\subsubsection*{K-Plus Proches Voisins}
\begin{itemize}
	\item \textbf{Nombre de voisins}: Valeur du nombre de voisins utilisé pour statuer le jeu de données.
	\item \textbf{Poids}: Mesure de poids utilisé pour la prédiction (\emph{uniform}, \emph{distance}). 
	\item \textbf{Algorithme}: Choix de l'algorithme utilisé pour déterminer les plus proches voisins (\emph{ball\_tree}, \emph{kd\_tree}, \emph{brute}, \emph{auto}).
	\item \textbf{Taille des feuilles}: Valeur affectant la vitesse de contruction et de recherche des voisins. Utilisé par les algorithmes \emph{ball\_tree} et \emph{kd\_tree}.
	\item \textbf{Puissance p}: Permet le choix de la métrique entre la distance de Manhattan et la distance Euclidienne.
\end{itemize}

\subsubsection*{Analyse du Discriminant Linéaire}
\begin{itemize}
	\item \textbf{Algorithme}: Choix de l'algorithme pour le calcul des distributions (\emph{svd}, \emph{lsqr}, \emph{eigen})
	\item \textbf{Nombre de composants}: Valeur déterminant le nombre de composants utilisé pour la réduction de dimensionalité.
	\item \textbf{Seuil}: Valeur du seuil spécifiquement utilisé pour l'algorithme \emph{svd}.
\end{itemize}

\subsubsection*{Bayes naïf gaussienne}
\begin{itemize}
	\item \textbf{Lissage des variables}: Valeur du lissage qui est une proportion de la variance devant être ajoutée afin de stabiliser les résultats des calculs.
\end{itemize}

\subsubsection*{Forêts aléatoires}
\begin{itemize}
	\item \textbf{Nombre d'estimateurs}: Valeur du nombre d'arbres utilisé pour construire l'arbre.
	\item \textbf{Profondeur maximale}: Valeur de la profondeur maximale d'un arbre.
\end{itemize}

\subsubsection*{Machine à vecteur de support}
\begin{itemize}
	\item \textbf{Paramètre de régularisarion C}: Valeur déterminant la force de régularisation qui est la proportionnelle inverse de C.
	\item \textbf{Noyau}: Choix du type de noyau utilisé par l'algorithme (\emph{linear}, \emph{poly}, \emph{rbf}, \emph{sigmoid})
	\item \textbf{Gamma}: Valeur du coefficient utilisé par le noyau, spécifiquement pour les noyaux \emph{poly}, \emph{rbf} et \emph{sigmoid}.
\end{itemize}

\subsection{Architecture des modèles}

    \begin{figure}[H]
        \centering
        %\includegraphics[width=15cm]{images/}
        \caption{Architecture d'un bloc dense}
        \label{fig:architecture_bloc_dense}
    \end{figure}

    
    \begin{figure}[H]
        \centering
        %\includegraphics[width=17cm]{images/}
        \caption{Architecture de l'auto-encodeur des caractéristiques}
        \label{fig:architecture_autoencoder_caracteristique}
    \end{figure}

    