\section{Choix des modèles de classification}
Le choix des différents classifieurs pour mener à bien ce projet a été réalisé dans l'optique d'essayer plusieurs familles de classifieurs. Nous nous sommes arrêtés à l'utilisation de six classifieurs multiclasses pour une application d'un apprentissage supervisée implémentés par la librairie Sklearn.

\subsubsection*{Perceptron MultiCouches}
Le réseau de neurones de type Perceptron MultiCouches a pour but d'apprendre à établir un lien mathématique entre les données de l'ensemble d'entrée et les différentes étiquettes de classe, dans le cas d'une classification. De part la possibilité de disposer de multiples couches, le réseau a la capacité d'établir des modèles non-linéaires, qui pourrait se révéler intéressant sur notre jeu de données. Nous décidons d'utiliser la méthode \emph{neural\_network.MLPClassifier} implémentée par Sklearn, qui utilise la rétro-propagation ainsi que la fonction d'erreur de l'entropie croisée. Aussi, étant donné notre problème multi-classe, la fonction d'activation de la couche de sortie est une Softmax.

\subsubsection*{K-Plus Proches Voisins}
La classification basée sur les voisins n'a pas comme but d'être la méthode la plus généralisable possible, elle se contente en effet de sauvegarder les exemples du jeu de données d'entraînement. Celle-ci peut cependant se révélée très performante et, au vu de la taille de notre jeu de données d'entraînement, son utilisation est envisageable.
Nous avons décidé de tester la méthode des K-Plus Proches Voisins, implémentés par Sklearn avec neighbors.KNeighborsClassifier.

\subsubsection*{Analyse du Discriminant Linéaire}
L'essai sur notre jeu de données d'entraînement de la méthode de l'analyse du discriminant linéaire permet de s'informer si ce jeu de données est effectivement bien linéairement séparable. Ce type de classification, étant de type \textcolor{red}{closed-form}, démontre son efficacité lors de la phase d'entraînement. La méthode est implémentée par Sklearn sous la librairie \emph{discriminant\_analysis.LinearDiscriminantAnalysis}.

\subsubsection*{Bayes naïf gaussienne}
Les méthodes de classification se basant sur l'application du théorème de Bayes font l'hypothèse qu'il exsite une indépendance conditionnelle entre chaque attribut sachant la valeur de l'étiquette de classe. L'utilisation d'une estimation par Maximum A Posteriori est aussi utilisée afin de déterminer les distributions de chacune des étiquettes de classe et de chacun des attributs en sachant la valeur de l'étiquette de classe. Nous avons décider, en plus, de faire l'hypothèse que les données suivent des distributions gaussiennens dans l'optique d'utiliser la méthode de classification de Bayes naïf gaussien. Celle-ci se retrouve implémentée par Sklearn sous la méthode \emph{naive\_bayes.GaussianNB}.

\subsubsection*{Forêts aléatoires}
Dans le but de généraliser au mieux, les méthodes d'ensemble permettent de combiner les prédictions de multiples estimateurs en un seul estimateur. On s'intéresse tout d'abord aux méthodes basées sur la moyenne des prédictions de chaque estimateur, dont les arbres de décision aléatoire font partis. 
La méthode Sklearn \emph{ensemble.RandomForestClassifier} a la particularité d'implémenter une méthode de type Boostrap, qui permet de décroître d'autant plus la variance de l'estimateur ayant souvent tendance à sur-apprendre.

\subsubsection*{Machine à vecteur de support}
Les méthodes de machines à vecteur de support sont également implémentées dans Sklearn et permettent d'apporter une approche "un-contre-un" à la classification. Celles-ci montrent aussi de bons résultats lorsque la dimensionalité des données est importante, ce qui pourrait être le cas de notre jeu de données d'entraînement. On utilise alors la méthode Sklearn \emph{svm.SVC} qui est spécifique à la classification.

\subsection{Choix des métriques}
??
